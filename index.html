<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <meta name="title" content="Feature-Gradient Attribution - Julius Šula">
  <meta name="description" content="Enhancing Vision Transformer explanations with Sparse Autoencoders. Demonstrates 10-44% faithfulness improvements across medical and natural imaging datasets.">
  <meta name="keywords" content="Vision Transformers, Sparse Autoencoders, Explainable AI, Attribution Methods, Medical Imaging, TransMM, Interpretability">
  <meta name="author" content="Julius Šula">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Julius Šula - Master's Thesis">
  <meta property="og:title" content="Feature-Gradient Attribution: Enhancing Vision Transformer Explanations with Sparse Autoencoders">
  <meta property="og:description" content="Enhancing Vision Transformer explanations with Sparse Autoencoders. Demonstrates 10-44% faithfulness improvements across medical and natural imaging datasets.">
  <meta property="og:url" content="https://piragi.github.io/thesis">
  <meta property="og:image" content="https://piragi.github.io/thesis/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="Feature-Gradient Attribution - Research Preview">
  <meta property="article:published_time" content="2025-01-01T00:00:00.000Z">
  <meta property="article:author" content="Julius Šula">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="Vision Transformers">
  <meta property="article:tag" content="Sparse Autoencoders">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@piragi">
  <meta name="twitter:creator" content="@piragi">
  <meta name="twitter:title" content="Feature-Gradient Attribution: Enhancing Vision Transformer Explanations with Sparse Autoencoders">
  <meta name="twitter:description" content="Enhancing Vision Transformer explanations with Sparse Autoencoders. Demonstrates 10-44% faithfulness improvements across medical and natural imaging datasets.">
  <meta name="twitter:image" content="https://piragi.github.io/thesis/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="Feature-Gradient Attribution - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="Feature-Gradient Attribution: Enhancing Vision Transformer Explanations with Sparse Autoencoders">
  <meta name="citation_author" content="Šula, Julius">
  <meta name="citation_publication_date" content="2025">
  <meta name="citation_conference_title" content="Master's Thesis">
  <meta name="citation_pdf_url" content="https://piragi.github.io/thesis/Feature_Gradient_Attribution_Thesis.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <title>Feature-Gradient Attribution - Julius Šula | Master's Thesis</title>

  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">

  <!-- MathJax for rendering LaTeX formulas -->
  <script>
    MathJax = {
      tex: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [['$$', '$$']],
        processEscapes: true
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" async></script>

  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "Feature-Gradient Attribution: Enhancing Vision Transformer Explanations with Sparse Autoencoders",
    "description": "Enhancing Vision Transformer explanations with Sparse Autoencoders. Demonstrates 10-44% faithfulness improvements across medical and natural imaging datasets.",
    "author": [
      {
        "@type": "Person",
        "name": "Julius Šula",
        "affiliation": {
          "@type": "Organization",
          "name": "TU Wien"
        }
      }
    ],
    "datePublished": "2025-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "Master's Thesis"
    },
    "url": "https://piragi.github.io/thesis",
    "image": "https://piragi.github.io/thesis/static/images/social_preview.png",
    "keywords": ["Vision Transformers", "Sparse Autoencoders", "Explainable AI", "Attribution Methods", "Medical Imaging"],
    "abstract": "Vision Transformers achieve strong performance in medical and natural imaging, yet their decision processes remain opaque. We introduce Feature-Gradient Attribution, which extends TransMM's principle from attention space to semantic feature space using Sparse Autoencoders.",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://piragi.github.io/thesis"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "Machine Learning Interpretability"
      },
      {
        "@type": "Thing",
        "name": "Computer Vision"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "TU Wien",
    "url": "https://www.tuwien.at",
    "sameAs": [
      "https://twitter.com/piragi",
      "https://github.com/piragi"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Feature-Gradient Attribution: Enhancing Vision Transformer Explanations with Sparse Autoencoders</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://github.com/piragi" target="_blank">Julius Šula</a></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">TU Wien<br>Master's Thesis 2025</span>
                    <span class="eql-cntrb"><small><br>Advisors: Prof. Thomas Lukasiewicz, Bayar Ilhan Menzat</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <span class="link-block">
                        <a href="Feature_Gradient_Attribution_Thesis.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Thesis PDF</span>
                      </a>
                    </span>

                  <span class="link-block">
                    <a href="https://github.com/piragi/featuregradientgate" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Vision Transformers achieve strong performance in medical and natural imaging, yet their decision processes remain opaque. TransMM, a leading attribution method, combines attention with gradients to highlight influential patches. We introduce Feature-Gradient Attribution, which extends TransMM's principle from attention space to semantic feature space using Sparse Autoencoders (SAEs). SAEs decompose activations into interpretable features; we project gradients onto this feature basis to compute feature-gradient scores capturing both feature presence and influence on predictions. These scores modulate TransMM's attention maps, forming a lightweight, semantically-informed correction.
          </p>
          <p>
            Across three datasets (chest X-rays, endoscopy, natural images), two architectures (fine-tuned ViT-B/16, CLIP ViT-B/32), and three complementary faithfulness metrics, our method achieves consistent improvements: 10.5–44.3% on SaCo, 14.0–43.0% on Faithfulness Correlation, and 1.3–10.8% on Pixel Flipping. Ablation studies confirm that both feature activations and gradients are necessary. To our knowledge, this is the first integration of sparse semantic features during attribution computation, demonstrating that mechanistic feature structure can materially enhance Transformer attributions.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method Overview</h2>
        
        <div class="content has-text-justified">
          <p>
            Standard attribution methods like TransMM identify important patches by combining <strong>Attention Value</strong> with <strong>Attention Gradient</strong>. 
            We extend this principle from "token space" to "semantic feature space".
          </p>
          <p>
            Instead of just asking <i>"where is the model looking?"</i>, we project gradients through a Sparse Autoencoder (SAE) to ask <i>"which semantic concepts are influencing the prediction?"</i>. 
            We calculate a <strong>Feature-Gradient Score</strong> by multiplying the activation of a feature by its gradient influence.
          </p>
        </div>

        <div class="box has-background-light">
          <div class="columns is-vcentered is-mobile">
            <div class="column is-half has-text-right-tablet has-text-centered-mobile border-right">
              <p class="is-size-7 is-uppercase has-text-grey has-text-weight-bold mb-1">Standard TransMM</p>
              <p class="is-family-monospace is-size-5">
                \( A \cdot \nabla A \)
              </p>
              <p class="is-size-7 mt-1">Attention Value × Gradient</p>
            </div>

            <div class="column is-half has-text-left-tablet has-text-centered-mobile">
              <p class="is-size-7 is-uppercase has-text-primary has-text-weight-bold mb-1">Our Method</p>
              <p class="is-family-monospace is-size-5 has-text-primary">
                \( f \cdot \nabla f \)
              </p>
              <p class="is-size-7 mt-1">Feature Activation × Gradient</p>
            </div>
          </div>
          
          <hr class="my-3" style="background-color: #dbdbdb;">

          <div class="content">
            <p class="is-size-6 mb-1">This semantic score creates a gate that modulates the attribution at specific layers:</p>
            <div class="is-size-5 is-family-monospace">
              $$ \text{Attr}_{\ell} = \text{TransMM}_{\ell} \times \text{Gate}(\mathbf{f} \cdot \nabla \mathbf{f}) $$
            </div>
          </div>
        </div>

        <div class="content has-text-justified mt-5">
          <p>
             The resulting gates amplify patches where relevant semantic features are both <strong>present</strong> (high activation) and <strong>influential</strong> (high gradient).
          </p>
          <div class="columns is-centered">
            <div class="column is-11">
              <figure class="image">
                 <img src="static/images/method_overview.png" alt="Feature-Gradient Attribution Method Overview">
                <figcaption class="has-text-grey is-size-6 mt-2">
                  <strong>Figure 1:</strong> System architecture. We extract residual stream activations, decompose them into sparse features using an SAE, and project gradients into this feature basis. The resulting scores modulate TransMM's attention maps before relevancy propagation.
                </figcaption>
              </figure>
            </div>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>





<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Quantitative Results</h2>
    <div class="content has-text-justified">
      <p>
        We evaluated our method across three diverse datasets using three complementary faithfulness metrics.
        <strong>SAE features consistently outperform</strong> both the baseline (Vanilla TransMM) and randomized controls,
        with statistical significance (\(p < 0.001\)) across most metrics.
      </p>
    </div>
    
    <div class="table-container">
      <table class="table is-striped is-hoverable is-fullwidth has-text-centered">
        <thead>
          <tr>
            <th class="has-text-left">Dataset</th>
            <th>Method</th>
            <th>SaCo <span class="icon is-small has-text-success"><i class="fas fa-arrow-up"></i></span></th>
            <th>Faithfulness Corr. <span class="icon is-small has-text-success"><i class="fas fa-arrow-up"></i></span></th>
            <th>Pixel Flipping <span class="icon is-small has-text-success"><i class="fas fa-arrow-down"></i></span></th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td class="has-text-left" rowspan="2" style="vertical-align: middle;"><strong>COVID-QU-Ex</strong><br><small>(Chest X-Ray)</small></td>
            <td class="has-text-left">Vanilla TransMM</td>
            <td>0.459</td>
            <td>0.309</td>
            <td>93.17</td>
          </tr>
          <tr class="is-selected">
            <td class="has-text-left"><strong>Ours</strong></td>
            <td><strong>0.507</strong> <small>(+10.5%)</small></td>
            <td><strong>0.442</strong> <small>(+43.0%)</small></td>
            <td><strong>83.06</strong> <small>(-10.8%)</small></td>
          </tr>
          
          <tr>
            <td class="has-text-left" rowspan="2" style="vertical-align: middle;"><strong>Hyperkvasir</strong><br><small>(Endoscopy)</small></td>
            <td class="has-text-left">Vanilla TransMM</td>
            <td>0.504</td>
            <td>0.479</td>
            <td>94.50</td>
          </tr>
          <tr class="is-selected">
            <td class="has-text-left"><strong>Ours</strong></td>
            <td><strong>0.727</strong> <small>(+44.3%)</small></td>
            <td><strong>0.550</strong> <small>(+14.8%)</small></td>
            <td><strong>93.31</strong> <small>(-1.3%)</small></td>
          </tr>

          <tr>
            <td class="has-text-left" rowspan="2" style="vertical-align: middle;"><strong>ImageNet</strong><br><small>(Natural)</small></td>
            <td class="has-text-left">Vanilla TransMM</td>
            <td>0.250</td>
            <td>0.314</td>
            <td>8.83</td>
          </tr>
          <tr class="is-selected">
            <td class="has-text-left"><strong>Ours</strong></td>
            <td><strong>0.337</strong> <small>(+34.8%)</small></td>
            <td><strong>0.358</strong> <small>(+14.0%)</small></td>
            <td><strong>8.52</strong> <small>(-3.5%)</small></td>
          </tr>
        </tbody>
      </table>
      <p class="is-size-7 has-text-grey has-text-centered">
        ↑ Higher is better. ↓ Lower is better. Comparisons against Vanilla TransMM baseline on test sets.
      </p>
    </div>
  </div>
</section>


<!-- Feature Visualizations -->
      <div class="<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Feature-Gradient Attribution Visualizations</h2>
      
      <div class="columns is-centered">
        <div class="column is-three-quarters content">
          <p>
            To understand how our method improves faithfulness, we isolated the <strong>top 100 images</strong> 
            that showed the highest composite improvement across all three metrics (SaCo, Faithfulness Correlation, and Pixel Flipping).
          </p>
          <p>
             We extracted the specific SAE latent feature responsible for the primary contribution in these samples. 
             Our analysis reveals that the method improves attribution via three primary strategies:
          </p>
          <ul>
            <li><strong>Suppression of Confounders:</strong> Reducing attention on correlated but non-causal features (e.g., faces in clothing datasets).</li>
            <li><strong>Artifact Removal:</strong> Filtering out watermarks, text, or border artifacts.</li>
            <li><strong>Context-Dependent Modulation:</strong> Switching a feature from a "booster" to a "suppressor" depending on the class context (e.g., fur texture).</li>
          </ul>
        </div>
      </div>

      <div class="is-divider" style="margin: 3rem auto; width: 50%; border-top: 1px solid #dbdbdb;"></div>

      <p class="subtitle has-text-centered mt-2">
        <strong>Explore the examples below:</strong> The visualizations show the original image, the baseline attribution, 
        our gated attribution, and the specific SAE feature prototype responsible for the correction.
      </p>
      <carousel-header">
      <!-- Simplified Carousel -->
      <div class="simple-carousel">
        <!-- Text Info Section -->
        <div class="carousel-info">
          <h3 class="carousel-title"></h3>
          <p class="carousel-description"></p>
        </div>

        <!-- Image Display with Side Navigation -->
        <div class="carousel-wrapper">
          <button class="carousel-nav carousel-prev" aria-label="Previous image">
            <i class="fas fa-chevron-left"></i>
          </button>

          <div class="carousel-image-container">
            <img id="carousel-image" src="" alt="Feature visualization" loading="lazy"/>
          </div>

          <button class="carousel-nav carousel-next" aria-label="Next image">
            <i class="fas fa-chevron-right"></i>
          </button>
        </div>

        <!-- Counter -->
        <div class="carousel-counter-container">
          <span class="carousel-counter"><span id="carousel-current">1</span> / <span id="carousel-total">0</span></span>
        </div>
      </div>

      <!-- Hidden carousel items for data extraction (will be removed later) -->
      <div id="results-carousel" class="carousel results-carousel" style="display: none;">
      <!-- Feature case studies with descriptions -->
      <div class="item" data-feature="10968">
        <h3 class="feature-title">Feature 10968, Layer 6 - Face Detector</h3>
        <p class="feature-description">Detects facial landmarks (eyes, nose) in both humans and animals. Suppresses face regions in clothing classes (e.g., Academic Gown) to redirect attention from people to the relevant objects being worn.</p>
        <img class="case-study-img" src="static/images/case_studies_feature_10968.png" alt="Case studies for feature 10968" loading="lazy"/>
        <img class="prototypes-img" src="static/images/prototypes_feature_10968.png" alt="Prototypes for feature 10968" loading="lazy"/>
      </div>
      <div class="item" data-feature="20254">
        <h3 class="feature-title">Feature 20254, Layer 6 - Fur Texture Detector</h3>
        <p class="feature-description">Detects animal fur patterns. Context-dependent: boosts attribution when fur defines the class (e.g., Schipperke) but suppresses generic fur texture to force discrimination via more specific features (e.g., facial geometry in similar dog breeds).</p>
        <img class="case-study-img" src="static/images/case_studies_feature_20254.png" alt="Case studies for feature 20254" loading="lazy"/>
        <img class="prototypes-img" src="static/images/prototypes_feature_20254.png" alt="Prototypes for feature 20254" loading="lazy"/>
      </div>
      <div class="item" data-feature="37778">
        <h3 class="feature-title">Feature 37778, Layer 6 - Text & Watermark Detector</h3>
        <p class="feature-description">Identifies text overlays and copyright labels. Suppresses these artifacts to prevent the model from using metadata as classification shortcuts, ensuring predictions rely on actual image content.</p>
        <img class="case-study-img" src="static/images/case_studies_feature_37778.png" alt="Case studies for feature 37778" loading="lazy"/>
        <img class="prototypes-img" src="static/images/prototypes_feature_37778.png" alt="Prototypes for feature 37778" loading="lazy"/>
      </div>
      <div class="item" data-feature="45529">
        <h3 class="feature-title">Feature 45529, Layer 9 - Human Face Context</h3>
        <p class="feature-description">Similar to Layer 6 face detection but specifically targets human presence in deeper layers. Suppresses human figures in object classification tasks where people are secondary (e.g., Cloak, Golden Retriever).</p>
        <img class="case-study-img" src="static/images/case_studies_feature_45529.png" alt="Case studies for feature 45529" loading="lazy"/>
        <img class="prototypes-img" src="static/images/prototypes_feature_45529.png" alt="Prototypes for feature 45529" loading="lazy"/>
      </div>
      <div class="item" data-feature="5436">
        <h3 class="feature-title">Feature 5436, Layer 10 - Background Noise Filter</h3>
        <p class="feature-description">Detects high-frequency edges, blur, and generic gradients. Acts as a broadband background filter by suppressing low-information patches across diverse classes, performing late-stage denoising of attribution maps.</p>
        <img class="case-study-img" src="static/images/case_studies_feature_5436.png" alt="Case studies for feature 5436" loading="lazy"/>
        <img class="prototypes-img" src="static/images/prototypes_feature_5436.png" alt="Prototypes for feature 5436" loading="lazy"/>
      </div>
      <div class="item" data-feature="6561">
        <h3 class="feature-title">Feature 6561, Layer 9 - Red Texture Detector</h3>
        <p class="feature-description">Identifies red background textures and colors. Suppresses correlated but non-causal color biases (e.g., red velvet backgrounds often seen with pianos) to focus on semantically meaningful regions.</p>
        <img class="case-study-img" src="static/images/case_studies_feature_6561.png" alt="Case studies for feature 6561" loading="lazy"/>
        <img class="prototypes-img" src="static/images/prototypes_feature_6561.png" alt="Prototypes for feature 6561" loading="lazy"/>
      </div>
      </div>
    </div>
  </div>
</section>
<!-- End feature visualizations -->




<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@mastersthesis{Sula2025FeatureGradient,
  title={Feature-Gradient Attribution: Enhancing Vision Transformer Explanations with Sparse Autoencoders},
  author={Šula, Julius},
  school={TU Wien},
  year={2025},
  url={https://piragi.github.io/thesis}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
